# -*- coding: utf-8 -*-
"""Group12_HW2 (4).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fIxFzPmtF62hS15o2tS8HuwwL9bkEFAv

## Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import OneHotEncoder
from sklearn.naive_bayes import CategoricalNB
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay

"""#Part 1 as model to check manula calculations!

## a) Naive Bayes Classifier

We have Car data provided in this link (https://docs.google.com/spreadsheets/d/11v5X6ZQw7YVL0EOpsdt4BWR3uwEz4YB5/edit?usp=drive_link) collected and the dataset contains three features.

Load Data, Drop extra column.
The target attribute is marked Stolen, which indicates whether a specific car is stolen or not.Assign the traget Y
"""

# Load the table
data = pd.read_excel('/content/Car Data.xlsx')

# Drop the "Example No." column
data = data.drop('Example No.', axis=1)

# Separate features (X) and target variable (y)
X = data.drop('Stolen', axis=1)
Y = data['Stolen']

data.head(10)

"""Encode the data. Apply one-hot encoding to the features (X)"""

# Apply one-hot encoding to the features (X)
encoder = OneHotEncoder(sparse=False)
X_encoded = encoder.fit_transform(X)

"""Train the Naive Bayes classifier
Using Naive Bayes Classifier to classify a new instance
"""

# Train the Naive Bayes classifier
naive_bayes = CategoricalNB()
naive_bayes.fit(X_encoded, Y)

"""New Instance = (Blue, SUV, Domestic).
Encode the instance
Predict into (Yes or No).
"""

# Define the new instance for classification
new_instance = pd.DataFrame({'Color': ['Blue'], 'Type': ['SUV'], 'Origin': ['Domestic']})

# Apply one-hot encoding to the new instance
new_instance_encoded = encoder.transform(new_instance)

# Make predictions for the new instance
predictions = naive_bayes.predict(new_instance_encoded)

# Display the predicted class
print("Predicted Class: ", predictions[0])

"""Please include the detailed calculation process.

**Calculations!!!!**

All Calculations are in the report

## b) The rejection area

Consider the loss table, which contains three actions and two classes. Calculate the expected risk of three actions, and determine the rejection area of P(Class1| x).
"""

# Convert the loss table to a NumPy array
loss_table = np.array([[0, 6], [3, 0], [2, 2]])

# Calculate the expected risk of each action
expected_risk = np.sum(loss_table, axis=1) / np.sum(loss_table)

# Determine the rejection area of P(Class1| x)
rejection_area = expected_risk[2]

# Display the expected risk and rejection area
print("Expected Risk:")
print("Action a1:", expected_risk[0])
print("Action a2:", expected_risk[1])

print("\n Rejection Area of P(Class1| x):", rejection_area)

"""#Part 2

## Load the data
"""

#loading the data
data = pd.read_csv('/content/spambase.data', header=None)
data.head()

"""## a) Split the dataset 1

Split the dataset into two parts as training data and test data. first 80 percent samples should be selected as training data and last 20 percent samples should be selected as test data.
"""

#splitting the data as reuested using data slicing technique and creating the classifiers
split_index = int(0.8 * len(data))

train_data_a = data[:split_index]
test_data_a = data[split_index:]

x_train_a = train_data_a.drop(columns=[57])
y_train_a = train_data_a[57]

x_test_a = test_data_a.drop(columns=[57])
y_test_a = test_data_a[57]

gnb = GaussianNB()
gnb.fit(x_train_a, y_train_a)

mnb = MultinomialNB()
mnb.fit(x_train_a, y_train_a)

"""Compute the confusion matrix and the accuracy of test data for Gaussian
and Multinomial Naive Bayes Classifiers
"""

#predicting the outputs and calculating the accuracy of the models and creating the confusion matrix for evaluation
y_pred_gnb_a = gnb.predict(x_test_a)
cm_gnb_a = confusion_matrix(y_test_a, y_pred_gnb_a)
accuracy_gnb_a = accuracy_score(y_test_a, y_pred_gnb_a)

y_pred_mnb_a = mnb.predict(x_test_a)
cm_mnb_a = confusion_matrix(y_test_a, y_pred_mnb_a)
accuracy_mnb_a = accuracy_score(y_test_a, y_pred_mnb_a)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))

disp_gnb_a = ConfusionMatrixDisplay(confusion_matrix=cm_gnb_a, display_labels=["Not Spam", "Spam"])
disp_gnb_a.plot(ax=axes[0], cmap='Blues', xticks_rotation='vertical')
axes[0].set_title("Gaussian Naive Bayes\nAccuracy: {:.2f}".format(accuracy_gnb_a))

disp_mnb_a = ConfusionMatrixDisplay(confusion_matrix=cm_mnb_a, display_labels=["Not Spam", "Spam"])
disp_mnb_a.plot(ax=axes[1], cmap='Blues', xticks_rotation='vertical')
axes[1].set_title("Multinomial Naive Bayes\nAccuracy: {:.2f}".format(accuracy_mnb_a))

plt.tight_layout()
plt.show()

"""## b) Split the dataset using train test split function

Use train test split function on input and output of the whole data and utilize 80% of samples as train and 20% of samples as test data.
"""

data.columns

x = data.drop(57,axis=1)
y = data[57]

x_train_b , x_test_b , y_train_b ,y_test_b = train_test_split(x,y,stratify = y, test_size=0.2, random_state=42)

# Initialize and train the guassian and Multinomial Naive Bayes classifier
gnb = GaussianNB()
gnb.fit(x_train_b,y_train_b)

mnb = MultinomialNB()
mnb.fit(x_train_b,y_train_b)

"""Compute the confusion matrix and the accuracy of test data for Gaussian and Multinomial Naive Bayes Classifiers"""

y_pred_gnb = gnb.predict(x_test_b)
cm_gnb = confusion_matrix(y_test_b, y_pred_gnb)
accuracy_gnb = accuracy_score(y_test_b, y_pred_gnb)

y_pred_mnb = mnb.predict(x_test_b)
cm_mnb = confusion_matrix(y_test_b, y_pred_mnb)
accuracy_mnb = accuracy_score(y_test_b, y_pred_mnb)

fig, axes = plt.subplots(1, 2, figsize=(12, 6))

disp_gnb = ConfusionMatrixDisplay(confusion_matrix=cm_gnb, display_labels=["Not Spam", "Spam"])
disp_gnb.plot(ax=axes[0], cmap='Blues', xticks_rotation='vertical')
axes[0].set_title("Gaussian Naive Bayes\nAccuracy: {:.2f}".format(accuracy_gnb))

disp_mnb = ConfusionMatrixDisplay(confusion_matrix=cm_mnb, display_labels=["Not Spam", "Spam"])
disp_mnb.plot(ax=axes[1], cmap='Blues', xticks_rotation='vertical')
axes[1].set_title("Multinomial Naive Bayes\nAccuracy: {:.2f}".format(accuracy_mnb))

plt.tight_layout()
plt.show()

"""## c)  Another Naive Bayes classifier

Use another Naive Bayes classifier of your choice to check for the improvement in terms of accuarcy score of test data in (b) over Gaussian and Multinomial asked in (b)
"""

# Initialize and train the Bernoulli Naive Bayes classifier
bern = BernoulliNB(alpha=1.0, fit_prior=True)
bern.fit(x_train_b, y_train_b)
y_pred_bern = bern.predict(x_test_b)

accuracy = accuracy_score(y_test_b, y_pred_bern)
print("Accuracy:", accuracy)

"""Provide an explanation for the improvement in performance (if any).

>The Bernoulli Naive Bayes classifier might have achieved better accuracy in the previous problem compared to Gaussian and Multinomial Naive Bayes classifiers due to the following reasons:
<br>1) Binary Feature Representation as it assume features are binary and it will suited with the binary feature representation.
<br>2) Handling Irrelevant Features as it considers the presence or absence of features, disregarding their frequency or intensity.

Provide classification report in terms of precision, recall and F1-score.
"""

#generating the classification report
report = classification_report(y_test_b, y_pred_bern)
print("Classification Report:\n", report)

"""Display the confusion matrix for only the selected classifier"""

#drawing the confusion matrix
cm_bern = confusion_matrix(y_test_b, y_pred_bern)
fig, ax = plt.subplots(figsize=(6, 4))
disp_mnb = ConfusionMatrixDisplay(confusion_matrix=cm_bern, display_labels=["Not Spam", "Spam"])
disp_mnb.plot(ax=ax, cmap='Blues', xticks_rotation='vertical')
ax.set_title("Bernoulli Naive Bayes\nAccuracy: {:.2f}".format(accuracy))
plt.tight_layout()
plt.show()

"""## d) Split the data into four equal parts

Take same first 80 percent as asked in (a) training samples and split the data
into four equal parts

according to order such as the first 25% of training data
(subset 1), the second 25% of training data (subset 2), the third 25% of training
data (subset 3) and the fourth 25% of training data (subset 4).
"""

#splitting the part A training data into 4 equal subsets
split_index = int(len(train_data_a))
subset_size = int(split_index * 0.25)

subset_1 = data[:subset_size]
subset_2 = data[subset_size:2*subset_size]
subset_3 = data[2*subset_size:3*subset_size]
subset_4 = data[3*subset_size:split_index]

x_subset_1 = subset_1.drop(columns=[57])
y_subset_1 = subset_1[57]

x_subset_2 = subset_2.drop(columns=[57])
y_subset_2 = subset_2[57]

x_subset_3 = subset_3.drop(columns=[57])
y_subset_3 = subset_3[57]

x_subset_4 = subset_4.drop(columns=[57])
y_subset_4 = subset_4[57]

"""Train selected classifier chosen in (c) for each subset and predict the accuracy score by evaluating on last 20 percent of test data assumed in (a)."""

#fitting the bernoulli model ont he 4 subsets and generating accuracy scores for each subset
accuracy_scores = []
for i in range(1,5):
  x = globals()['x_subset_'+str(i)]
  y = globals()['y_subset_'+str(i)]
  bern.fit(x,y)
  y_pred = bern.predict(x_test_a)
  accuracy_scores.append(accuracy_score(y_test_a,y_pred))

"""Plot bar chart to show all subsetsâ€™ accuracy on the figure."""

#plotting bar plot for the accuracy scores of the 4 subsets
subset_labels = ['Subset 1', 'Subset 2', 'Subset 3', 'Subset 4']

plt.bar(subset_labels, accuracy_scores)
plt.xlabel('Subset')
plt.ylabel('Accuracy')
plt.title('Accuracy Scores for Subsets')
plt.ylim(0, 1)
plt.show()

"""### Add your comment

> As we can see:
<br>1)The first subset training data set didn't provide the model with a variation between the spam and not spam classes as the first portion of the data are all from '1' class.
<br>2)For the second subset it did contain a small portion of the '0' class so the model was able to capture some more info to classify better and did increase the accuracy .
<br>3) For the third and fourth subset the accuracy was 100% because the test data is all from the '0' class as the subsets so they were most likely to assign all the coming predictions to class '0'.
"""